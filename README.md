This project implements a face recognition system using the Vision Transformer (ViT) architecture trained on the VGGFace2 dataset. The VGGFace2 dataset is a large-scale face dataset collected by the Visual Geometry Group from the University of Oxford, containing over 3.3 million images from more than 9,000 unique identities. The dataset is structured such that each identity has a dedicated folder containing multiple face images, which is ideal for classification tasks. In this project, each folder name represents a unique identity class, and the model is trained to predict the correct identity given a facial image.

The Vision Transformer model used in this project is loaded via the timm library (e.g., vit_base_patch16_224). Images are resized (typically to 224x224), normalized, and augmented using standard PyTorch and torchvision utilities. The model is trained using the cross-entropy loss function and optimized with the AdamW optimizer. A learning rate scheduler (such as cosine annealing or step decay) can also be used to improve convergence.

Training and evaluation scripts are provided to handle data loading, model training, checkpoint saving, and validation. The training script supports customization for batch size, number of epochs, learning rate, and the number of classes (equal to the number of unique identities). After training, the model can be evaluated on a separate validation set to compute top-1 and top-5 accuracy scores. An inference script is also available to predict the identity of a face image using the trained model checkpoint.

This project uses Python 3 and PyTorch, along with libraries like torchvision, timm, scikit-learn, and OpenCV. The overall pipeline includes dataset preparation, model training, evaluation, and inference, providing a complete deep learning solution for face recognition using state-of-the-art transformer-based models. The code is modular and well-structured, making it easy to adapt for fine-tuning on other datasets or integrating into larger applications.
